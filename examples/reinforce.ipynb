{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colleen.mcmillon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules import Module\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Replay video\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose cpu or gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  4\n",
      "Sample observation [-9.1713071e-01  2.9960743e+38 -2.5674871e-01  2.6299537e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the policy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, layers_data: list):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_size = s_size\n",
    "        for size, activation in layers_data:\n",
    "            self.layers.append(nn.Linear(input_size, size))\n",
    "            input_size = size\n",
    "            if activation is not None:\n",
    "                assert isinstance(activation, Module), \\\n",
    "                    \"Each tuple should contain a layer size (int) and an activation (ex. nn.ReLU()).\"\n",
    "                self.layers.append(activation)\n",
    "        self.layers.append(nn.Linear(size, a_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        ### ADD IN REQUISITE THAT THE ACTION IS POSSIBLE\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training loop\n",
    "class REINFORCE:\n",
    "    def __init__(self, env, policy, optimizer, gamma, model_path='./saved_model'):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.model_path = model_path\n",
    "\n",
    "\n",
    "    def load_saved_model(self):\n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "                if os.path.isfile(self.model_path+'/reinforce.pth'):\n",
    "                    self.policy.load_state_dict(torch.load(self.model_path+'/reinforce.pth'))\n",
    "                    print(\"Loaded saved model\")\n",
    "        else:\n",
    "            print(\"No saved model\")\n",
    "\n",
    "\n",
    "    def train(self, n_training_episodes, max_t, print_every=100, save_model=True, use_saved_model=False):\n",
    "        # Help calculate score during training\n",
    "        # Line 3 of pseudocode\n",
    "        self.max_t = max_t\n",
    "        scores_deque = deque(maxlen=100)\n",
    "        scores = []\n",
    "\n",
    "        if use_saved_model:\n",
    "            self.load_saved_model()\n",
    "\n",
    "        # Repeat: Generate an episode, calculte the return based on the steps that remain, calculate loss, update gradient using loss\n",
    "        for i_episode in range(1, n_training_episodes+1):\n",
    "            self.saved_log_probs = []\n",
    "            rewards = []\n",
    "            state = self.env.reset()[0]\n",
    "            # Generate an episode following the policy\n",
    "            for t in range(max_t):\n",
    "                action, log_prob = self.policy.act(state)\n",
    "                self.saved_log_probs.append(log_prob)\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "            scores_deque.append(sum(rewards))\n",
    "            scores.append(sum(rewards))\n",
    "\n",
    "            # Calculate the returns\n",
    "            returns = self.calculate_returns(rewards)\n",
    "\n",
    "            # Update the policy\n",
    "            self.update_policy(returns)\n",
    "\n",
    "            if i_episode % print_every == 0:\n",
    "                print('Episode{}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if save_model:\n",
    "            os.makedirs(self.model_path, exist_ok=True)\n",
    "            torch.save(self.policy.state_dict(), self.model_path+'/reinforce.pth')\n",
    "            print(\"Saved model to disk\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def calculate_returns(self, rewards):\n",
    "        # Calculate the return\n",
    "        returns = deque(maxlen=self.max_t)\n",
    "        n_steps = len(rewards) \n",
    "                \n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(self.gamma*disc_return_t + rewards[t])\n",
    "        # Standardize returns to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        # eps is the smallest representable float which is added to the standard\n",
    "        # deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean())/(returns.std() + eps)\n",
    "\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def update_policy(self, returns):\n",
    "        # Calculate the loss\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Calculate the gradient and update the weights\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "    def play(self, max_steps, n_eval_episodes, use_saved_model=True):\n",
    "        \"\"\"\n",
    "        Play for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "        :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "        :param max_steps: Max number of steps in episode\n",
    "        \"\"\" \n",
    "        if use_saved_model:\n",
    "            self.load_saved_model()\n",
    "            \n",
    "        self.policy.eval()  \n",
    "        episode_rewards = []\n",
    "        for episode in range(n_eval_episodes):\n",
    "            state = self.env.reset()[0]\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards_ep = 0\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action, _ = self.policy.act(state)\n",
    "                new_state, reward, done, info, _ = self.env.step(action)\n",
    "                total_rewards_ep += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                state = new_state\n",
    "            episode_rewards.append(total_rewards_ep)\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)  \n",
    "        print(f'Average reward: {mean_reward}, Standard deviation: {std_reward}')  \n",
    "\n",
    "        return mean_reward, std_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "    \"layers_data\": [(16, nn.ReLU())],\n",
    "    \"n_training_episodes\": 300,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "    \"save_model\": True, \n",
    "    \"model_path\": './saved_model',\n",
    "    \"print_every\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"layers_data\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = REINFORCE(env, cartpole_policy, cartpole_optimizer, cartpole_hyperparameters[\"gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colleen.mcmillon/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode100\tAverage Score: 73.51\n",
      "Episode200\tAverage Score: 224.86\n",
      "Episode300\tAverage Score: 266.16\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce.train(cartpole_hyperparameters[\"n_training_episodes\"], cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"print_every\"], use_saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model\n",
      "Average reward: 836.0, Standard deviation: 134.08952233489387\n"
     ]
    }
   ],
   "source": [
    "avg, stddev = reinforce.play(cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
